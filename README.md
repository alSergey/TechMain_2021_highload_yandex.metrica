# Тема проекта - Яндекс Метрика

## 1. Тема и целевая аудитория

### MVP

1. Сбок метрик сайта (сырые метрики без отчетов)

### Целевая аудитория

* У Яндекс Метрики целевая аудитория - владельцы сайтов, которые хотят собирать статистику.
* Яндекс Метрика установлена на 6.7 % всех сайтов в интернете. [На 2020 в интернете примерно 1.8 млд сайтов](https://sdvv.ru/articles/elektronnaya-kommertsiya/statistika-interneta-2020-sayty-domeny-khosting-trafik/), значит Яндекс Метрика установлена примерно на 120 млн сайтов.
* Сервисом пользуются по всему миру. В основном используют в Европе и РФ [3 по популярности система веб аналитики в Европе](https://w3techs.com/technologies/details/ta-yandexmetrika)

## 2. Расчет нагрузки

### Продуктовые метрики

1. **Количество сайтов** - 120 млн
2. **Средний размер хранилища пользователя** - за пользователя будем считать владельца сайта. Метриками сайта - будем считать информацию о клиентах и их посещений. Для удобства метрики клиента будем считать в хитах.

    * Профиль и информация о сайте имеют очень малый размер, ими можно принебречь.

3. **Средний размер хранилища клиента**
    * Информация о клиенте

    | Общие данные               | Размер |
    | -------------------------- | ------ |
    | ClienID                    | 5 Б    |
    | Возраст                    | 5 Б    |
    | Пол                        | 5 Б    |
    | Первый визит               | 10 Б   |
    | Последний визит            | 10 Б   |
    | Количество визитов         | 5 Б    |
    | Общее время на сайте       | 10 Б   |
    | **Общий размер**           | 50 Б   |

    * Информация о клиенте за 1 хит

    | Данные каждого хита   | Размер |
    | --------------------- | ------ |
    | ClienID               | 10 Б   |
    | Номер визита          | 10 Б   |
    | Таймзона              | 10 Б   |
    | Дата визита           | 10 Б   |
    | Время визита          | 10 Б   |
    | Время на сайте        | 10 Б   |
    | Источник трафика      | 10 Б   |
    | Домен                 | 10 Б   |
    | URL                   | 10 Б   |
    | Страна                | 10 Б   |
    | Регион                | 10 Б   |
    | Операционная система  | 10 Б   |
    | Браузер               | 10 Б   |
    | Язык браузера         | 10 Б   |
    | Разрешение экрана     | 10 Б   |
    | User Agent            | 10 Б   |
    | IP                    | 10 Б   |
    | Тип устройства        | 10 Б   |
    | Модель устройства     | 10 Б   |
    | Событие               | 10 Б   |
    | Общий размер          | 200 Б  |

    Это не все метрики, которые приходят. Я выделил основные, болше данных можно посмотреть [тут](https://play.clickhouse.com). В среднем клиент на сайте совершает 1,5 хита в день. Подсчитаем нагрузку сайта за день.

    **Размер метрик за одни день**

    | Размер хита | Количество пользователей | Количество хитов | Общее хитов | Общий размер |
    | ----------- | ------------------------ | ---------------- | ----------- | ------------ |
    | 200 Б       | 50                       | 1,5              | 75          | 15 КБ        |

4. **Среднее количество действий пользователя**
    * Информация о действиях пользователя настолько мала, что мы можем ей принебречь (Авторизация, Регистрация, Регистрация сайта, Просмотр метрик).

5. **Среднее количество действий клиента**
    * Под действиями клиента понимаем заход на сайт, нажатие на кнопки, переход между страницами и тд. Все это метрики сайта. Для удобства действия клиентов будем считать в хитам не конкретизируя их. В среднем клиент совершает 1,5 хита в день. Cреднее количество клиентов 50, получаем 75 хитов в день.

### Технические метрики

1. **Размер хранения в разбивке по типам данных**
    * Информация о клиентах - мы посчитали, что информация о клиенте занимает 50Б. Среднее количество клиентов в день на сайте 200, будем считать что за все время количество уникальных пользователей было в 5 раз больше, тогда нам нужно `120 млн * 1000 * 50Б = 6 ТБ`
    * Метрики сайта - у нас 120 млн сайтов, Яндекс Метрика существует с 2009 года, то есть 12 лет. Так как раньше сайтов было меньше, то будем считать среднее количество сайтов за 12 лет 30 млн, тогда нам нужно `30 млн * 12 * 365 * 75 * 15 КБ = 150 ПБ`

    В ClickHouse используется сжатие данных для эффективного хранения данных. Для того, чтобы понять какое будет сжатие я загрузил [тестовый набор данных](https://clickhouse.com/docs/ru/getting-started/example-datasets/metrica/) Яндекс метрики размеров 8 ГБ. После загрузки в ClickHouse и оптимизаций таблица весила 520 МБ. Получается размер сжатия примерно 15 раз.

    * Сжатая информация о клиентах - `0,4 ТБ`
    * Сжатая метрика сайта - `10 ПБ`

2. RPS и трафик в секунду

    | Количество хитов в день | Размер хита | RPS    | Трафик   |
    | ----------------------- | ----------- | ------ | -------- |
    | 75                      | 200 Б       | 100000 | 20 МБ/C  |

## 3. Логическая схема

Сущности:

* Пользователь (User)
* Вебсайт (Website)
* Вебсайты пользователя (UserWebsite)
* Клиент (Client)
* Визит клиента (Hit)

![Логическая схема базы данных](./assets/logic.png)

## 4. Физическая схема

Так как таблицы User, Website, UserWebsite не будут иметь небольшую нагрузку и будут обновляться достаточно редко, то будем их хранить в PostgreSQL.

Таблица Hit, Client будут обновляться достаточно часто и в них будет храниться много данных, для них используем ClickHouse. Для взаимодействия ClickHouse и PostgreSQL будем использовать [внешние словари](https://clickhouse.com/docs/ru/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources/#dicts-external_dicts_dict_sources-postgresql)

У нас таблица Hit будет очень большой, поэтому будем ее шардировать между серверами. Плюс ClickHouse в том, что он из коробки позволяет шардировать таблицы и объединять сервера в [кластер](https://clickhouse.com/docs/ru/getting-started/tutorial/#cluster-deployment), и работать с кластером через [distributed table](https://clickhouse.com/docs/ru/engines/table-engines/special/distributed/). Distributed table - это прослойка, которая знает обо всех шардах и репликах и умеет общаяться с ними. Для этого ClickHouse использует решение [ZooKeeper](https://zookeeper.apache.org). Ключом шардирования сделаем домен сайта, чтобы все данные по сайту попытаться хранить в одном месте. Если нам не будет хватать скорости, то мы сможем увеличить количество сервером для distributed table и скорость возрастет. Подробная информация и метрики приведены [тут](https://habr.com/ru/post/509540/). Для обеспечения отказоустойчивости для каждого шарда будет своя реплика. Таким образом мы будем иметь копию данных каждого шарда. Статья про шардирование в [ClickHouse](https://habr.com/ru/company/smi2/blog/317682/).

Разделим нашу логическую схему на следующие базы:

* PostgreSQL

![PostgreSQL](./assets/postgresql.png)

* ClickHouse

![ClickHouse](./assets/clickhouse.png)

## 5. Технологии

### Frontend

Как такового фронтенда метрик у нас не будет. Любой сайт, на котором установлены метрики будет нашим фронтендом метрки. Туда просто добавляется наш JS скрипт, который будет собирать данные и отправлять на бекенд.

### Backend

Бекенд митрик должен получать данные и передавать distributed table, который дальше уже будет распределять данные между шардами. Он может выполнять минимальные преобразования по данным.

### Трейсинг/Логгинг/Мониторинг

#### Трейсинг

* Для того, чтобы отслеживать где у нас упал запрос при похоже между бекендами будем использовать Jagger.

#### Логгинг

* Для логгирования бекенда будем использовать Graylog.

#### Мониторинг

* Для мониторинга бекенда будем использовать prometheus + grafana.

### Балансировка

* Для балансировки между ДЦ будем использовать Geo-DNS балансировку, которая позволит нам обеспечить наименьшую задержку для пользователя.
* Для балансировки между бекендами внутри ДЦ будем использовать Nginx L7 балансировку.

## 6. Схема проекта

![Метрика](./assets/schema_metrics.png)

ДЦ будет состоять из Nginx, бекенда, серверов кластера ClickHouse.

DNS используя Geo-DNS балансировку сбалансирует нас в ДЦ, внутри него будет осуществляться балансировка Nginx L7 между бекендами.

ClickHouse у нас будет объединен в один кластер для того, чтобы нам не нужно было задумывать о том в какой шард нам нужно идти. Для работы с кластером в каждом ДЦ будет установлен сервер с distributed table, который будет общаться с кластером. Подробнее в разделе про базы данных в разделе "Физическая схема".

В PostgreSQL у нас храниться информация о владельцах сайтов и самих сайтах, поэтому в данном случае рассматривать это не будем. Наш бекенд занимается сбором метрик. Для работы с PostgreSQL должен быть написан отдельный сервис.

## 7. Список серверов

Найти точного географического распределения Яндекс Метрики не удалось, поэтому будем предполагать, что она очень популярна в странах СНГ, чуть менее популярна в Европе и США. Поэтому расположим пару ДЦ в России, один в Европе и один в США.

Для работы ClickHouse нам нужно очень много постоянной памяти, чтобы хранить все хиты. Возьмем диски объемом 8 ТБ. На сервере будем иметь 8 таких дисков. Рассчитаем количество сереров `10 ПБ / 8 дисков * 8 ТБ = 160 серверов`. Как мы уже раньше сказали для надежной работы будет иметь 1 реплику под каждый шард. Для обработки большого количества запросов нам нужно много CPU и RAM.

| CPU (ядер) | RAM (ГБ) | HDD (ТБ) | Количество       |
| ---------- | -------- | -------- | ---------------- |
| 32         | 64       | 4 х 8    | 160 + 160 реплик |

Для работы с кластером ClickHouse в каждом ДЦ установим сервер с distributed table. Он будет принимать запросы от бека и асинхронно передавать на кластера ClickHouse. Сервер не будет хранить данные, а будет передавать на кластера, поэтому диска ему много не нужно, но нужна оператива, чтобы эти данные хранить и потом передавать на кластера.

| CPU (ядер) | RAM (ГБ) | SSD (ГБ) | Количество  |
| ---------- | -------- | -------- | ----------- |
| 32         | 256      | 256      | В каждом ДЦ |

Nginx будет иметь огромную нагрузку из-за большого количества запросов. В среднем Nginx выдает 300 RPS на 1 ядро для HTTPS. Возьмем 16 ядерный процессор. В нашем случае нам нужно `100 000 RPS / 16 ядер * 300 RPS = 22` сервера nginx + возьмем 22 резервных. Для обработки большого количества соединений нам нужно много RAM, а вот размер диска нам не очень важен.

| CPU (ядер) | RAM (ГБ) | SSD (ГБ) | Количество     |
| ---------- | -------- | -------- | -------------- |
| 16         | 32       | 64       | 22 + 22 резерв |

Если посмотреть примеры [тестов go](https://habr.com/ru/post/324818/), то можно увидеть, что он выдает достаточно много RPS. Учитывая большие нагрузки, то будем считать что у нас будет 5 000 RPS. Тогда нам нужно `100 000 RPS / 5 000 RPS = 20`. Нам нужно достаточно много CPU и памяти для обработки такого количества запросов, а вот диска нам много не надо.

| CPU (ядер) | RAM (ГБ) | SSD (ТБ) | Количество     |
| ---------- | -------- | -------- | -------------- |
| 24         | 64       | 64       | 20 + 20 резерв |

## 8. Литература

1. <https://ru.wikipedia.org/wiki/Яндекс.Метрика>
2. <https://sdvv.ru/articles/elektronnaya-kommertsiya/statistika-interneta-2020-sayty-domeny-khosting-trafik/>
3. <https://w3techs.com/technologies/details/ta-yandexmetrika>
4. <https://play.clickhouse.com>
5. <https://clickhouse.com/docs/ru/getting-started/example-datasets/metrica/>
6. <https://clickhouse.com/docs/ru/getting-started/tutorial/#cluster-deployment>
7. <https://clickhouse.com/docs/ru/engines/table-engines/special/distributed/>
8. <https://clickhouse.com/docs/ru/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources/#dicts-external_dicts_dict_sources-postgresql>
9. <https://habr.com/ru/post/322724/>
10. <https://habr.com/ru/post/509540/>
11. <https://habr.com/ru/company/smi2/blog/317682/>
12. <https://habr.com/ru/post/324818/>
13. <https://zookeeper.apache.org>
